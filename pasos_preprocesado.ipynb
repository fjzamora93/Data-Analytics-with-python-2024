{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos a seguir en el preprocesado de datos\n",
    "\n",
    "Corregir, completar, Creaci√≥n y Conversi√≥n.\n",
    "Limpieza de datos, selecci√≥n de caracter√≠sticas, extracci√≥n de caracter√≠sticas, transformaci√≥n y creaci√≥n.\n",
    "\n",
    "## Obligatorios\n",
    "\n",
    "1. Tratar valores faltantes o err√≥neos\n",
    "\n",
    "    a. borrar fila (si tenemos suficientes registros)\n",
    "\n",
    "    b. borrar la columna (si faltan demasiados valores en esa columna)\n",
    "\n",
    "    c. imputar (SimpleImputer: media, mediana o moda)\n",
    "\n",
    "\n",
    "2. Tratar los Outliers.\n",
    "\n",
    "    a. IsolationForest - bastante buenos, no asumen ninguna distribuci√≥n en concreto de los datos.\n",
    "\n",
    "    b. IQR (Interquartile Range) - si sobrepasa el 1.5 por encima del tercero o por debajo del primero, va fuera.\n",
    "\n",
    "\n",
    "3. Manipulaci√≥n de datos (escalarlos):\n",
    "\n",
    "    a. Estandarizar (todos los datos de una columna en la misma escala)\n",
    "\n",
    "    b. Normalizer (todos los datos de una fila en la misma escala)\n",
    "\n",
    "    c. Transformar las distribuciones (????????????????????????????????????)\n",
    "\n",
    "4. Reducci√≥n de la dimensionalidad (siempre despu√©s de escalar los datos)\n",
    "\n",
    "    a. SelectKBest (te quedas con las m√°s importantes)\n",
    "    \n",
    "    b. Eliminaci√≥n de las poco relevantes (RFE).\n",
    "\n",
    "    c. M√©todos del √Årbol de Decisi√≥n.\n",
    "\n",
    "\n",
    "## Opcionales \n",
    "\n",
    "\n",
    "5. Transformaci√≥n de categ√≥ricos a num√©ricos (equivalente al pd.get_dummies):\n",
    "    a. LabelEncoder (para la salida)\n",
    "    b. OrdinalEncoder (para la entrada)\n",
    "    c. OneHotEncoder\n",
    "\n",
    "6. Creaci√≥n de nuevas features significativas (PCA) -despu√©s de escalar y convertir los categ√≥ricos.\n",
    "\n",
    "\n",
    "7. Discretizaci√≥n de los datos (binning)\n",
    "    a. Binarizer\n",
    "    b. KBinsDiscretizer\n",
    "\n",
    "\n",
    "8. Corregir desbalanceo de datos y sesgos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tratar valores faltantes o err√≥neos \n",
    "\n",
    "Utilizaremos un m√©todo espec√≠fico para primero detectar los NaN, y, una vez los tengamos identificados, ver cu√°l es la mejor soluci√≥n.\n",
    "Lo podemos hacer con algunos de estos:\n",
    "\n",
    "a. SimpleImputer (mean, median, most_frequent, constant)\n",
    "\n",
    "b. InterativeImputer (utiliza una regresi√≥n para rellenar, por defecto una bayesiana). Tiene la peculairidad de que aplica el fit y el transform por separado.\n",
    "\n",
    "c. KNNImputer (utiliza el KNN para rellenar)\n",
    "\n",
    "d. A tomar por culo. Nos cargamos la columna o las filas y santas pascuas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/student-por.csv', sep=';')\n",
    "df.isnull().sum() #te va a sacar los nulos de cada columna\n",
    "\n",
    "df = df.drop('famrel', axis=1) #A tomar por culo la columna, como famrel ten√≠a m√°s de 400 nulos, se va a la puta.\n",
    "\n",
    "df.age = df.age.apply(lambda value: np.nan if value == '?' else value)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_simple_age = pd.DataFrame(df_simple['age']) # 2 dimensiones\n",
    "\n",
    "imputer.fit(df_simple_age)\n",
    "df_simple['age'] = imputer.transform(df_simple_age)\n",
    "df_simple['age'] = df_simple['age'].astype(int)\n",
    "df_simple.head(15)\n",
    "\n",
    "\n",
    "# Verificar que ahora la columna age tiene 0 nulos:\n",
    "df_simple.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tratar Outliers\n",
    "\n",
    "Observaremos las caracter√≠sticas de los Outliers y decidiremos si perjudican al modelo. De ser as√≠, debemos ocuparnos de ellos. \n",
    "En este caso, una forma muy eficaz de tratar los outliers es con IsolationForest, ya que no asume ning√∫n tipo de distribuci√≥n. Aunque como alternativa tambi√©n est√° el IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOLATION FOREST\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "housing = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['MedHouseVal'] = housing.target\n",
    "\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# A nivel interno, el entrenamiento asigna  una etiqueta de 1 a los datos normales y una etiqueta de -1 a los outliers\n",
    "# iso = IsolationForest(contamination=0.01) # 0 a 0.5\n",
    "iso = IsolationForest()\n",
    "y_outliers = iso.fit_predict(X)\n",
    "y_outliers\n",
    "\n",
    "#Obtenem os el filtro, recordamos que est√°n etiquetados los datos.\n",
    "filter = y_outliers != -1\n",
    "\n",
    "X_wo, y_wo = X[filter, :], y[filter]\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"X_wo shape: \", X_wo.shape)\n",
    "print(\"Outliers borrados: \", X.shape[0] - X_wo.shape[0])\n",
    "\n",
    "#Finalmente sacar√≠amos las m√©tricas (esto ser√≠a una funci√≥n con las m√©tricas, veriamos que al quitar Outliers la cosa mejora)\n",
    "calc_predictions(X_wo, y_wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IQR, reexaminar todo el c√≥dio del IQR que est√° en 2.PREPROCESADOS_T√âCNICAS\n",
    "\n",
    "from collections import Counter\n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"EG\n",
    "    outlier_indices = []\n",
    "\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
    "\n",
    "        # append the found outlier indices for col to the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n",
    "\n",
    "    return multiple_outliers\n",
    "\n",
    "housing = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['MedHouseVal'] = housing.target\n",
    "\n",
    "outliers = detect_outliers(df, 2, df.drop('MedHouseVal', axis=1).columns) #Detectamos los Outliers\n",
    "df.loc[outliers] # Los mostramos por pantalla\n",
    "df = df.drop(outliers, axis=0).reset_index(drop=True) # Los quitamos del axis 0, es decir, todos los registros de outliers\n",
    "\n",
    "X = df.drop(['MedHouseVal'], axis=1)\n",
    "y = df['MedHouseVal']\n",
    "calc_predictions(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Escalado de datos\n",
    "\n",
    "### 3.1 Normalizer\n",
    "Normalizer trata cada registro de forma INDIVIDUAL y pasa todos sus datos a la misma escala.\n",
    "Esto es especialmente √∫til para ver qu√© peso tiene cada caracter√≠stica en cada registro de manera proporcional.\n",
    "Imag√≠nate este ejemplo:\n",
    "\n",
    "Supongamos que tenemos una serie de documentos, y queremos intentar averiguar qu√© tem√°tica tienen. En este caso, no podemos atenernos al n√∫mero de veces qu√© aparece cada palabra, sino en qu√© proporci√≥n aparece cada palabra documento. \n",
    "\n",
    "Si utiliz√°semos un MinMaxScaler normal, el resultado ser√≠a que aquellos documentos m√°s largos parecer√≠a que tratan ciertos temas en mayor proporci√≥n que ciertos documentos de poca extensi√≥n. Aunque es probable que un documento de menor extensi√≥n aborde dicho tema en mayor profundidad. Para abordar esta situaci√≥n, utilizar√≠amos un Normalizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MinMaxScaler\n",
    "\n",
    "El valor m√°s bajo valdr√≠a 0, el valor m√°s alto valdr√≠a 1. Todos los dem√°s quedar√≠an entre medias. El problema del MinMaxScaler es que suprime el efecto de los Outliers, y eso puede ser un problema. Si este es el caso, otra opci√≥n ser√≠a utilizar StandarSCaller. Aqu√≠, la desviaci√≥n est√°ndar puede variar, no es como en el StandarScaler que siempre dar√° 1 de desviaci√≥n est√°ndar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  ],\n",
       "       [0.25, 0.25],\n",
       "       [0.5 , 0.5 ],\n",
       "       [0.75, 0.75],\n",
       "       [1.  , 1.  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 200], [2, 300], [3, 400], [4, 500], [5, 600]])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "minmax_data = scaler.fit_transform(data)\n",
    "\n",
    "minmax_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 StandarScaller\n",
    "El StandardScaler estandariza las caracter√≠sticas eliminando la media y escalando a una desviaci√≥n est√°ndar de 1. Esto significa que cada caracter√≠stica en los datos transformados tendr√° una media de 0 y una desviaci√≥n est√°ndar de 1. Eso quiere decir que habr√° valores que quedar√°n por encima de 1 o por debajo de -1, ya que los datos no est√°n restringidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.41421356, -1.41421356],\n",
       "       [-0.70710678, -0.70710678],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.70710678,  0.70710678],\n",
       "       [ 1.41421356,  1.41421356]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.array([[1, 200], [2, 300], [3, 400], [4, 500], [5, 600]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "standardized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Eliminaci√≥n de features poco relevantes (SelectKBest)\n",
    "\n",
    "Existen varios m√©todos para quedarnos con las features m√°s relevantes. Los que podemos comentar son los siguientes:\n",
    "\n",
    "1. SelectKBest: Este m√©todo selecciona las ùëò mejores caracter√≠sticas bas√°ndose en alguna funci√≥n de puntuaci√≥n estad√≠stica. Puedes elegir diferentes criterios de puntuaci√≥n, como (chi-cuadrado) para datos categ√≥ricos, ANOVA F-value para datos continuos, entre otros.\n",
    "\n",
    "2. M√©todos basados en √°rboles de decisi√≥n: Los √°rboles de decisi√≥n, como los √°rboles de decisi√≥n individuales, los bosques aleatorios (Random Forests) y los modelos de boosting como XGBoost, tienen un atributo llamado feature_importances_ que proporciona una puntuaci√≥n de importancia para cada caracter√≠stica. Estas puntuaciones pueden usarse para seleccionar las caracter√≠sticas m√°s relevantes.\n",
    "\n",
    "3. RFE (Recursive Feature Elimination): Este es un m√©todo iterativo que ajusta un modelo y elimina las caracter√≠sticas m√°s d√©biles hasta alcanzar el n√∫mero deseado de caracter√≠sticas. RFE utiliza un estimador (como una regresi√≥n lineal o un √°rbol de decisi√≥n) para evaluar la importancia de cada caracter√≠stica en cada iteraci√≥n.\n",
    "\n",
    "El m√©todo por defecto de SelectKBest en ScikitLearn es el valor F de ANOVA (ANOVA F-value). Este m√©todo se usa para seleccionar las mejores caracter√≠sticas bas√°ndose en el valor F de ANOVA entre cada caracter√≠stica y la variable objetivo. El valor F de ANOVA mide la relaci√≥n de varianza entre los grupos a la varianza dentro de los grupos, lo que es √∫til para datos continuos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Seleccionar las 2 mejores caracter√≠sticas utilizando ANOVA F-value (por defecto)\n",
    "selector = SelectKBest(k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Obtener los nombres de las caracter√≠sticas seleccionadas\n",
    "selected_features = selector.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_features]\n",
    "\n",
    "# Convertir las caracter√≠sticas seleccionadas en un DataFrame\n",
    "df_selected = pd.DataFrame(X_new, columns=selected_feature_names)\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Realizar un describe() en el DataFrame\n",
    "df_selected.describe()\n",
    "iris_df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Codificaci√≥n de variables categ√≥ricas (LabelEncoder)\n",
    "\n",
    "Esto es el equivalente al pd.get_dummies(), pero aqu√≠ vamos a hacerlo con m√©todos propios de sklearn.\n",
    "Concretamente, el que vamos a ver es el LabelEncoder (salida) y OrdinalEconder (entrada):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL ENCONDER: PARA LA SALIDA\n",
    "\n",
    "df = pd.read_csv(\"../data/sonar.csv\")\n",
    "\n",
    "X = df.drop('label', axis=1) #separamos la X\n",
    "y = df['label'] #separamos la y, que es la que vamos a aplicar el labelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# pasar y de categ√≥rico a num√©rico.\n",
    "# OrdinalEncoder se utiliza en las entradas (X)\n",
    "# LabelEncoder se utiliza en la salida (y)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Codificaci√≥n de variables categ√≥ricas (OrdinalEncoder / LabelEncoder)\n",
    "\n",
    "Para codificaci√≥n de los categ√≥ricos, tenemos 3 modelos:\n",
    "\n",
    "1. OrdinalEncoder. Se utiliza con los valores de entrada, la X. Por lo general, el proceso es aplicar el OrdinalEconder y despu√©s concatenar estos datos con los originales.\n",
    "\n",
    "2. LabelEncoder. Se utiliza √∫nicamente con los valores de salida, la y.\n",
    "\n",
    "3. OneHotEncoder. Es igual que el get_dummies() de pandas. Coge el categ√≥rico y crea tantas columnas como categor√≠as haya. Si se cumple la condic√≥n pone 1, si no se cumple, pone 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Supongamos que tenemos el siguiente DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'edad': [23, 45, 31],\n",
    "    'tama√±o_categorico': ['peque√±o', 'mediano', 'grande'],\n",
    "    'ingreso': [50000, 60000, 55000],\n",
    "    'label': ['A', 'B', 'A']\n",
    "})\n",
    "\n",
    "# Identificar las columnas categ√≥ricas y num√©ricas\n",
    "columnas_categoricas = ['tama√±o_categorico']\n",
    "columnas_numericas = ['edad', 'ingreso']\n",
    "\n",
    "# Separar las columnas categ√≥ricas y num√©ricas\n",
    "X_categorico = df[columnas_categoricas]\n",
    "X_numerico = df[columnas_numericas]\n",
    "\n",
    "# Transformar las columnas categ√≥ricas\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_categorico_encoded = ordinal_encoder.fit_transform(X_categorico)\n",
    "\n",
    "# CREAR UN NUEVO DATAFRAME CON LA TRANSFORMACI√ìN DE LAS CATEG√ìRICAS!!!\n",
    "X_categorico_encoded_df = pd.DataFrame(X_categorico_encoded, columns=columnas_categoricas)\n",
    "\n",
    "# CONCATENAMOS NUESTRAS NUM√âRICAS ORIGINALES CON LAS QUE HEMOS TRANSFORMADO HACE UN SEGUNDO\n",
    "X_final = pd.concat([X_numerico, X_categorico_encoded_df], axis=1)\n",
    "\n",
    "# Transformar la variable de salida con LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "print(X_final)\n",
    "print(y_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 An√°lisis de los Compomentes Principales (PCA) - Despu√©s de escalar y codificar los categ√≥ricos\n",
    "\n",
    "\n",
    "Es imperativo mencionar que un conjunto de caracter√≠sticas debe normalizarse antes de aplicar PCA. Por ejemplo, si un conjunto de caracter√≠sticas tiene datos expresados en unidades de kilogramos, a√±os luz o millones, la escala de variaci√≥n es enorme en el conjunto de entrenamiento. Si se aplica PCA en un conjunto de caracter√≠sticas de este tipo, las cargas resultantes para las caracter√≠sticas con una varianza alta tambi√©n ser√°n grandes. Por lo tanto, los componentes principales estar√°n sesgados hacia caracter√≠sticas con una gran variaci√≥n, lo que generar√° resultados falsos. Para ello escalamos los datos con: MinMaxScaler o StandardScaler.\n",
    "\n",
    "Finalmente, el √∫ltimo punto a recordar antes de comenzar a codificar es que PCA es una t√©cnica estad√≠stica y solo se puede aplicar a datos num√©ricos. Por lo tanto, las caracter√≠sticas categ√≥ricas deben convertirse en caracter√≠sticas num√©ricas antes de poder aplicar PCA.\n",
    "\n",
    "\n",
    "La PCA busca ordenar las caracter√≠sticas desde las que provocan mayor varianza hasta las que menos. \n",
    "\n",
    "Una alta varianza se considera m√°s importante en el contexto de PCA porque indica la direcci√≥n en la que los datos est√°n m√°s dispersos, y capturar esta varianza permite retener la mayor cantidad de informaci√≥n posible al reducir las dimensiones del conjunto de datos. Este enfoque asegura que las caracter√≠sticas m√°s importantes y la estructura subyacente de los datos se mantengan al simplificar el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Distribuci√≥n de datos\n",
    "\n",
    "Para evitar sesgos, puede plantearse preprocesar la distribuci√≥n de datos. Dentro de este apartado existen varias t√©cnicas, como CunatileTransformer, PowerTransformer o BoxCox. Nosotros nos vamos a centrar en CuantileTransformer.\n",
    "\n",
    "QuantileTransformer transforma las caracter√≠sticas de modo que sigan una distribuci√≥n espec√≠fica al utilizar cuantiles. Esto implica que se mapean los datos originales a nuevos valores utilizando sus posiciones en la distribuci√≥n acumulada (CDF). Esto tiene el efecto de eliminar la influencia de la forma original de la distribuci√≥n de los datos, resultando en datos que siguen la distribuci√≥n deseada.\n",
    "\n",
    "### Principales Par√°metros y Opciones\n",
    "1. output_distribution: Este par√°metro define la distribuci√≥n de salida. Las opciones son:\n",
    "\n",
    "'uniform': Transforma los datos para que sigan una distribuci√≥n uniforme.\n",
    "'normal': Transforma los datos para que sigan una distribuci√≥n gaussiana (normal).\n",
    "n_quantiles: N√∫mero de cuantiles a utilizar para dividir los datos. M√°s cuantiles pueden llevar a una transformaci√≥n m√°s precisa, pero a costa de mayor tiempo de computaci√≥n.\n",
    "\n",
    "2. subsample: Si el n√∫mero de muestras en los datos es mayor que este valor, se utilizar√° una muestra aleatoria de las primeras subsample muestras para ajustar la transformaci√≥n.\n",
    "\n",
    "3. random_state: Semilla para la generaci√≥n de n√∫meros aleatorios, √∫til para reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='feature1', ylabel='Count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'feature1': np.random.exponential(scale=2, size=1000),  # Distribuci√≥n exponencial\n",
    "    'feature2': np.random.normal(loc=0, scale=1, size=1000) # Distribuci√≥n normal\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Inicializar el QuantileTransformer\n",
    "transformer = QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=0)\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "df_transformed = transformer.fit_transform(df)\n",
    "\n",
    "# Convertir de nuevo a DataFrame para facilitar la visualizaci√≥n\n",
    "df_transformed = pd.DataFrame(df_transformed, columns=df.columns)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame transformado\n",
    "df_transformed.describe()\n",
    "\n",
    "#Y aqu√≠ vemos c√≥mo lo que iba a ser una distribuci√≥n exponencial, se convierte en guasina\n",
    "sns.histplot(df_transformed['feature1'], kde=True, color='blue', label='Original')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Descritizaci√≥n\n",
    "\n",
    "Conviertes variables continuas en variables discretas (rangos). Las dos que se utilizan con ScikitLearn son KbinsDiscretizer y Binarizer. El binarizer no lo vamos a explicar ya que √∫nicamente transformar en 0 o en 1. Es decir, o s√≠ o no entra en la categor√≠a. Esto mismo se puede conseguir con un KbinsDiscretizer de n_bins=2. Pero puede ser √∫til repasar el Binarizer en el futuro.\n",
    "\n",
    "## KbinsDiscretizer\n",
    "\n",
    "La discretizaci√≥n con KBinsDiscretizer puede ser √∫til en situaciones donde se necesita convertir caracter√≠sticas num√©ricas en categor√≠as para su uso en modelos que requieren variables categ√≥ricas. Al crear una instancia de KBinsDiscretizer, se especifican varios par√°metros importantes:\n",
    "\n",
    "### n_bins: \n",
    "Especifica el n√∫mero de bins (contenedores) en los que se discretizar√° la variable. Puedes especificar un √∫nico valor entero para bins de igual anchura o una lista para bins de igual frecuencia.\n",
    "\n",
    "### strategy: \n",
    "Determina la estrategia de discretizaci√≥n. Puede ser:\n",
    "\n",
    "-'uniform': Todos los bins tienen la misma anchura.\n",
    "\n",
    "-'quantile': Todos los bins tienen la misma cantidad de puntos de datos.\n",
    "\n",
    "-'kmeans': Los bins se forman utilizando el algoritmo de K-means.\n",
    "\n",
    "\n",
    "### encode: \n",
    "\n",
    "Especifica c√≥mo codificar las variables discretizadas. Puede ser 'ordinal' para codificar como n√∫meros enteros o 'onehot' para codificar como variables dummy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = np.array([[0, 2], [1, 3], [2, 4], [3, 5]])\n",
    "\n",
    "# Crear el discretizador\n",
    "enc = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='uniform')\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "X_binned = enc.fit_transform(X)\n",
    "\n",
    "X_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Creaci√≥n de nuesvas features o eliminaci√≥n manual\n",
    "\n",
    "Este paso normalmente es m√°s manual, ya que responde a un an√°lisis de los datos que tengamos. Por ejemplo, en el titanic, podemos crear una variable de 'viaja solo' si el tama√±o de la familia es == 1.\n",
    "\n",
    "O si hubiese demasiadas categor√≠as, podr√≠amos simplificar las categor√≠as por \"otros\" para aquellas que no tienen demasiados registros. Esto puede simplificar los datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
