{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos a seguir en el preprocesado de datos\n",
    "\n",
    "Corregir, completar, Creación y Conversión.\n",
    "Limpieza de datos, selección de características, extracción de características, transformación y creación.\n",
    "\n",
    "## Obligatorios\n",
    "\n",
    "1. Tratar valores faltantes o erróneos\n",
    "\n",
    "    a. borrar fila (si tenemos suficientes registros)\n",
    "\n",
    "    b. borrar la columna (si faltan demasiados valores en esa columna)\n",
    "\n",
    "    c. imputar (SimpleImputer: media, mediana o moda)\n",
    "\n",
    "\n",
    "2. Tratar los Outliers.\n",
    "\n",
    "    a. IsolationForest - bastante buenos, no asumen ninguna distribución en concreto de los datos.\n",
    "\n",
    "    b. IQR (Interquartile Range) - si sobrepasa el 1.5 por encima del tercero o por debajo del primero, va fuera.\n",
    "\n",
    "\n",
    "3. Manipulación de datos (escalarlos):\n",
    "\n",
    "    a. Estandarizar (todos los datos de una columna en la misma escala)\n",
    "\n",
    "    b. Normalizer (todos los datos de una fila en la misma escala)\n",
    "\n",
    "    c. Transformar las distribuciones (????????????????????????????????????)\n",
    "\n",
    "4. Reducción de la dimensionalidad (siempre después de escalar los datos)\n",
    "\n",
    "    a. SelectKBest (te quedas con las más importantes)\n",
    "    \n",
    "    b. Eliminación de las poco relevantes (RFE).\n",
    "\n",
    "    c. Métodos del Árbol de Decisión.\n",
    "\n",
    "\n",
    "## Opcionales \n",
    "\n",
    "\n",
    "5. Transformación de categóricos a numéricos (equivalente al pd.get_dummies):\n",
    "    a. LabelEncoder (para la salida)\n",
    "    b. OrdinalEncoder (para la entrada)\n",
    "    c. OneHotEncoder\n",
    "\n",
    "6. Creación de nuevas features significativas (PCA) -después de escalar y convertir los categóricos.\n",
    "\n",
    "\n",
    "7. Discretización de los datos (binning)\n",
    "    a. Binarizer\n",
    "    b. KBinsDiscretizer\n",
    "\n",
    "\n",
    "8. Corregir desbalanceo de datos y sesgos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tratar valores faltantes o erróneos \n",
    "\n",
    "Utilizaremos un método específico para primero detectar los NaN, y, una vez los tengamos identificados, ver cuál es la mejor solución.\n",
    "Lo podemos hacer con algunos de estos:\n",
    "\n",
    "a. SimpleImputer (mean, median, most_frequent, constant)\n",
    "\n",
    "b. InterativeImputer (utiliza una regresión para rellenar, por defecto una bayesiana). Tiene la peculairidad de que aplica el fit y el transform por separado.\n",
    "\n",
    "c. KNNImputer (utiliza el KNN para rellenar)\n",
    "\n",
    "d. A tomar por culo. Nos cargamos la columna o las filas y santas pascuas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/student-por.csv', sep=';')\n",
    "df.isnull().sum() #te va a sacar los nulos de cada columna\n",
    "\n",
    "df = df.drop('famrel', axis=1) #A tomar por culo la columna, como famrel tenía más de 400 nulos, se va a la puta.\n",
    "\n",
    "df.age = df.age.apply(lambda value: np.nan if value == '?' else value)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_simple_age = pd.DataFrame(df_simple['age']) # 2 dimensiones\n",
    "\n",
    "imputer.fit(df_simple_age)\n",
    "df_simple['age'] = imputer.transform(df_simple_age)\n",
    "df_simple['age'] = df_simple['age'].astype(int)\n",
    "df_simple.head(15)\n",
    "\n",
    "\n",
    "# Verificar que ahora la columna age tiene 0 nulos:\n",
    "df_simple.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tratar Outliers\n",
    "\n",
    "Observaremos las características de los Outliers y decidiremos si perjudican al modelo. De ser así, debemos ocuparnos de ellos. \n",
    "En este caso, una forma muy eficaz de tratar los outliers es con IsolationForest, ya que no asume ningún tipo de distribución. Aunque como alternativa también está el IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOLATION FOREST\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "housing = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['MedHouseVal'] = housing.target\n",
    "\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# A nivel interno, el entrenamiento asigna  una etiqueta de 1 a los datos normales y una etiqueta de -1 a los outliers\n",
    "# iso = IsolationForest(contamination=0.01) # 0 a 0.5\n",
    "iso = IsolationForest()\n",
    "y_outliers = iso.fit_predict(X)\n",
    "y_outliers\n",
    "\n",
    "#Obtenem os el filtro, recordamos que están etiquetados los datos.\n",
    "filter = y_outliers != -1\n",
    "\n",
    "X_wo, y_wo = X[filter, :], y[filter]\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"X_wo shape: \", X_wo.shape)\n",
    "print(\"Outliers borrados: \", X.shape[0] - X_wo.shape[0])\n",
    "\n",
    "#Finalmente sacaríamos las métricas (esto sería una función con las métricas, veriamos que al quitar Outliers la cosa mejora)\n",
    "calc_predictions(X_wo, y_wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IQR, reexaminar todo el códio del IQR que está en 2.PREPROCESADOS_TÉCNICAS\n",
    "\n",
    "from collections import Counter\n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"EG\n",
    "    outlier_indices = []\n",
    "\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
    "\n",
    "        # append the found outlier indices for col to the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n",
    "\n",
    "    return multiple_outliers\n",
    "\n",
    "housing = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['MedHouseVal'] = housing.target\n",
    "\n",
    "outliers = detect_outliers(df, 2, df.drop('MedHouseVal', axis=1).columns) #Detectamos los Outliers\n",
    "df.loc[outliers] # Los mostramos por pantalla\n",
    "df = df.drop(outliers, axis=0).reset_index(drop=True) # Los quitamos del axis 0, es decir, todos los registros de outliers\n",
    "\n",
    "X = df.drop(['MedHouseVal'], axis=1)\n",
    "y = df['MedHouseVal']\n",
    "calc_predictions(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Escalado de datos\n",
    "\n",
    "### 3.1 Normalizer\n",
    "Normalizer trata cada registro de forma INDIVIDUAL y pasa todos sus datos a la misma escala.\n",
    "Esto es especialmente útil para ver qué peso tiene cada característica en cada registro de manera proporcional.\n",
    "Imagínate este ejemplo:\n",
    "\n",
    "Supongamos que tenemos una serie de documentos, y queremos intentar averiguar qué temática tienen. En este caso, no podemos atenernos al número de veces qué aparece cada palabra, sino en qué proporción aparece cada palabra documento. \n",
    "\n",
    "Si utilizásemos un MinMaxScaler normal, el resultado sería que aquellos documentos más largos parecería que tratan ciertos temas en mayor proporción que ciertos documentos de poca extensión. Aunque es probable que un documento de menor extensión aborde dicho tema en mayor profundidad. Para abordar esta situación, utilizaríamos un Normalizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MinMaxScaler\n",
    "\n",
    "El valor más bajo valdría 0, el valor más alto valdría 1. Todos los demás quedarían entre medias. El problema del MinMaxScaler es que suprime el efecto de los Outliers, y eso puede ser un problema. Si este es el caso, otra opción sería utilizar StandarSCaller. Aquí, la desviación estándar puede variar, no es como en el StandarScaler que siempre dará 1 de desviación estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  ],\n",
       "       [0.25, 0.25],\n",
       "       [0.5 , 0.5 ],\n",
       "       [0.75, 0.75],\n",
       "       [1.  , 1.  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 200], [2, 300], [3, 400], [4, 500], [5, 600]])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "minmax_data = scaler.fit_transform(data)\n",
    "\n",
    "minmax_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 StandarScaller\n",
    "El StandardScaler estandariza las características eliminando la media y escalando a una desviación estándar de 1. Esto significa que cada característica en los datos transformados tendrá una media de 0 y una desviación estándar de 1. Eso quiere decir que habrá valores que quedarán por encima de 1 o por debajo de -1, ya que los datos no están restringidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.41421356, -1.41421356],\n",
       "       [-0.70710678, -0.70710678],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.70710678,  0.70710678],\n",
       "       [ 1.41421356,  1.41421356]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.array([[1, 200], [2, 300], [3, 400], [4, 500], [5, 600]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "standardized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Eliminación de features poco relevantes (SelectKBest)\n",
    "\n",
    "Existen varios métodos para quedarnos con las features más relevantes. Los que podemos comentar son los siguientes:\n",
    "\n",
    "1. SelectKBest: Este método selecciona las 𝑘 mejores características basándose en alguna función de puntuación estadística. Puedes elegir diferentes criterios de puntuación, como (chi-cuadrado) para datos categóricos, ANOVA F-value para datos continuos, entre otros.\n",
    "\n",
    "2. Métodos basados en árboles de decisión: Los árboles de decisión, como los árboles de decisión individuales, los bosques aleatorios (Random Forests) y los modelos de boosting como XGBoost, tienen un atributo llamado feature_importances_ que proporciona una puntuación de importancia para cada característica. Estas puntuaciones pueden usarse para seleccionar las características más relevantes.\n",
    "\n",
    "3. RFE (Recursive Feature Elimination): Este es un método iterativo que ajusta un modelo y elimina las características más débiles hasta alcanzar el número deseado de características. RFE utiliza un estimador (como una regresión lineal o un árbol de decisión) para evaluar la importancia de cada característica en cada iteración.\n",
    "\n",
    "El método por defecto de SelectKBest en ScikitLearn es el valor F de ANOVA (ANOVA F-value). Este método se usa para seleccionar las mejores características basándose en el valor F de ANOVA entre cada característica y la variable objetivo. El valor F de ANOVA mide la relación de varianza entre los grupos a la varianza dentro de los grupos, lo que es útil para datos continuos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Seleccionar las 2 mejores características utilizando ANOVA F-value (por defecto)\n",
    "selector = SelectKBest(k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Obtener los nombres de las características seleccionadas\n",
    "selected_features = selector.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_features]\n",
    "\n",
    "# Convertir las características seleccionadas en un DataFrame\n",
    "df_selected = pd.DataFrame(X_new, columns=selected_feature_names)\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Realizar un describe() en el DataFrame\n",
    "df_selected.describe()\n",
    "iris_df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Codificación de variables categóricas (LabelEncoder)\n",
    "\n",
    "Esto es el equivalente al pd.get_dummies(), pero aquí vamos a hacerlo con métodos propios de sklearn.\n",
    "Concretamente, el que vamos a ver es el LabelEncoder (salida) y OrdinalEconder (entrada):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL ENCONDER: PARA LA SALIDA\n",
    "\n",
    "df = pd.read_csv(\"../data/sonar.csv\")\n",
    "\n",
    "X = df.drop('label', axis=1) #separamos la X\n",
    "y = df['label'] #separamos la y, que es la que vamos a aplicar el labelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# pasar y de categórico a numérico.\n",
    "# OrdinalEncoder se utiliza en las entradas (X)\n",
    "# LabelEncoder se utiliza en la salida (y)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Codificación de variables categóricas (OrdinalEncoder / LabelEncoder)\n",
    "\n",
    "Para codificación de los categóricos, tenemos 3 modelos:\n",
    "\n",
    "1. OrdinalEncoder. Se utiliza con los valores de entrada, la X. Por lo general, el proceso es aplicar el OrdinalEconder y después concatenar estos datos con los originales.\n",
    "\n",
    "2. LabelEncoder. Se utiliza únicamente con los valores de salida, la y.\n",
    "\n",
    "3. OneHotEncoder. Es igual que el get_dummies() de pandas. Coge el categórico y crea tantas columnas como categorías haya. Si se cumple la condicón pone 1, si no se cumple, pone 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Supongamos que tenemos el siguiente DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'edad': [23, 45, 31],\n",
    "    'tamaño_categorico': ['pequeño', 'mediano', 'grande'],\n",
    "    'ingreso': [50000, 60000, 55000],\n",
    "    'label': ['A', 'B', 'A']\n",
    "})\n",
    "\n",
    "# Identificar las columnas categóricas y numéricas\n",
    "columnas_categoricas = ['tamaño_categorico']\n",
    "columnas_numericas = ['edad', 'ingreso']\n",
    "\n",
    "# Separar las columnas categóricas y numéricas\n",
    "X_categorico = df[columnas_categoricas]\n",
    "X_numerico = df[columnas_numericas]\n",
    "\n",
    "# Transformar las columnas categóricas\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_categorico_encoded = ordinal_encoder.fit_transform(X_categorico)\n",
    "\n",
    "# CREAR UN NUEVO DATAFRAME CON LA TRANSFORMACIÓN DE LAS CATEGÓRICAS!!!\n",
    "X_categorico_encoded_df = pd.DataFrame(X_categorico_encoded, columns=columnas_categoricas)\n",
    "\n",
    "# CONCATENAMOS NUESTRAS NUMÉRICAS ORIGINALES CON LAS QUE HEMOS TRANSFORMADO HACE UN SEGUNDO\n",
    "X_final = pd.concat([X_numerico, X_categorico_encoded_df], axis=1)\n",
    "\n",
    "# Transformar la variable de salida con LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "print(X_final)\n",
    "print(y_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Análisis de los Compomentes Principales (PCA) - Después de escalar y codificar los categóricos\n",
    "\n",
    "\n",
    "Es imperativo mencionar que un conjunto de características debe normalizarse antes de aplicar PCA. Por ejemplo, si un conjunto de características tiene datos expresados en unidades de kilogramos, años luz o millones, la escala de variación es enorme en el conjunto de entrenamiento. Si se aplica PCA en un conjunto de características de este tipo, las cargas resultantes para las características con una varianza alta también serán grandes. Por lo tanto, los componentes principales estarán sesgados hacia características con una gran variación, lo que generará resultados falsos. Para ello escalamos los datos con: MinMaxScaler o StandardScaler.\n",
    "\n",
    "Finalmente, el último punto a recordar antes de comenzar a codificar es que PCA es una técnica estadística y solo se puede aplicar a datos numéricos. Por lo tanto, las características categóricas deben convertirse en características numéricas antes de poder aplicar PCA.\n",
    "\n",
    "\n",
    "La PCA busca ordenar las características desde las que provocan mayor varianza hasta las que menos. \n",
    "\n",
    "Una alta varianza se considera más importante en el contexto de PCA porque indica la dirección en la que los datos están más dispersos, y capturar esta varianza permite retener la mayor cantidad de información posible al reducir las dimensiones del conjunto de datos. Este enfoque asegura que las características más importantes y la estructura subyacente de los datos se mantengan al simplificar el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Distribución de datos\n",
    "\n",
    "Para evitar sesgos, puede plantearse preprocesar la distribución de datos. Dentro de este apartado existen varias técnicas, como CunatileTransformer, PowerTransformer o BoxCox. Nosotros nos vamos a centrar en CuantileTransformer.\n",
    "\n",
    "QuantileTransformer transforma las características de modo que sigan una distribución específica al utilizar cuantiles. Esto implica que se mapean los datos originales a nuevos valores utilizando sus posiciones en la distribución acumulada (CDF). Esto tiene el efecto de eliminar la influencia de la forma original de la distribución de los datos, resultando en datos que siguen la distribución deseada.\n",
    "\n",
    "### Principales Parámetros y Opciones\n",
    "1. output_distribution: Este parámetro define la distribución de salida. Las opciones son:\n",
    "\n",
    "'uniform': Transforma los datos para que sigan una distribución uniforme.\n",
    "'normal': Transforma los datos para que sigan una distribución gaussiana (normal).\n",
    "n_quantiles: Número de cuantiles a utilizar para dividir los datos. Más cuantiles pueden llevar a una transformación más precisa, pero a costa de mayor tiempo de computación.\n",
    "\n",
    "2. subsample: Si el número de muestras en los datos es mayor que este valor, se utilizará una muestra aleatoria de las primeras subsample muestras para ajustar la transformación.\n",
    "\n",
    "3. random_state: Semilla para la generación de números aleatorios, útil para reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='feature1', ylabel='Count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'feature1': np.random.exponential(scale=2, size=1000),  # Distribución exponencial\n",
    "    'feature2': np.random.normal(loc=0, scale=1, size=1000) # Distribución normal\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Inicializar el QuantileTransformer\n",
    "transformer = QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=0)\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "df_transformed = transformer.fit_transform(df)\n",
    "\n",
    "# Convertir de nuevo a DataFrame para facilitar la visualización\n",
    "df_transformed = pd.DataFrame(df_transformed, columns=df.columns)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame transformado\n",
    "df_transformed.describe()\n",
    "\n",
    "#Y aquí vemos cómo lo que iba a ser una distribución exponencial, se convierte en guasina\n",
    "sns.histplot(df_transformed['feature1'], kde=True, color='blue', label='Original')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Descritización\n",
    "\n",
    "Conviertes variables continuas en variables discretas (rangos). Las dos que se utilizan con ScikitLearn son KbinsDiscretizer y Binarizer. El binarizer no lo vamos a explicar ya que únicamente transformar en 0 o en 1. Es decir, o sí o no entra en la categoría. Esto mismo se puede conseguir con un KbinsDiscretizer de n_bins=2. Pero puede ser útil repasar el Binarizer en el futuro.\n",
    "\n",
    "## KbinsDiscretizer\n",
    "\n",
    "La discretización con KBinsDiscretizer puede ser útil en situaciones donde se necesita convertir características numéricas en categorías para su uso en modelos que requieren variables categóricas. Al crear una instancia de KBinsDiscretizer, se especifican varios parámetros importantes:\n",
    "\n",
    "### n_bins: \n",
    "Especifica el número de bins (contenedores) en los que se discretizará la variable. Puedes especificar un único valor entero para bins de igual anchura o una lista para bins de igual frecuencia.\n",
    "\n",
    "### strategy: \n",
    "Determina la estrategia de discretización. Puede ser:\n",
    "\n",
    "-'uniform': Todos los bins tienen la misma anchura.\n",
    "\n",
    "-'quantile': Todos los bins tienen la misma cantidad de puntos de datos.\n",
    "\n",
    "-'kmeans': Los bins se forman utilizando el algoritmo de K-means.\n",
    "\n",
    "\n",
    "### encode: \n",
    "\n",
    "Especifica cómo codificar las variables discretizadas. Puede ser 'ordinal' para codificar como números enteros o 'onehot' para codificar como variables dummy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = np.array([[0, 2], [1, 3], [2, 4], [3, 5]])\n",
    "\n",
    "# Crear el discretizador\n",
    "enc = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='uniform')\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "X_binned = enc.fit_transform(X)\n",
    "\n",
    "X_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Creación de nuesvas features o eliminación manual\n",
    "\n",
    "Este paso normalmente es más manual, ya que responde a un análisis de los datos que tengamos. Por ejemplo, en el titanic, podemos crear una variable de 'viaja solo' si el tamaño de la familia es == 1.\n",
    "\n",
    "O si hubiese demasiadas categorías, podríamos simplificar las categorías por \"otros\" para aquellas que no tienen demasiados registros. Esto puede simplificar los datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
